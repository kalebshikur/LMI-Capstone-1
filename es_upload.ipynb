{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Splitting & ElasticSearch Uploads\n",
    "## LMI Capstone Team\n",
    "## Summer Chambers | Steve Morris | Kaleb Shikur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rule Splitting, Uploading to ES\n",
    "'''\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "# from requests_aws4auth import AWS4Auth\n",
    "import numpy as np\n",
    "import requests #gets urls\n",
    "# from aws_requests_auth.boto_utils import BotoAWSRequestsAuth\n",
    "\n",
    "host = 'https://search-lmi-capstone-test-bbewbwsli4uchb7r24zujwrqne.us-east-1.es.amazonaws.com'\n",
    "# auth = BotoAWSRequestsAuth(aws_host=host,\n",
    "#                            aws_region='us-east-1',\n",
    "#                            aws_service='es')\n",
    "# awsauth = AWS4Auth(YOUR_ACCESS_KEY, YOUR_SECRET_KEY, REGION, 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dedbbb462bf60acdb6c63621c8bf117c', 'cluster_name': '846033058400:lmi-capstone-test', 'cluster_uuid': 'vhqZptiWQEq0BVgaN6qi8Q', 'version': {'number': '7.8.0', 'build_flavor': 'oss', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2020-10-15T22:09:28.519938Z', 'build_snapshot': False, 'lucene_version': '8.5.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(host)\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "print(es.info())\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     http_auth=auth,\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "# print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Rule Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Rule/CMS-2018-0101-0001.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headers(rule_url):\n",
    "\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    rulechunks = {}\n",
    "    \n",
    "    startdict = {'a2':['2. proposals for modified participation options under 5-year agreement periods', '3. creating a basic track with glide path to performance-based risk'], \\\n",
    "     'a3':['3. creating a basic track with glide path to performance-based risk', '4. permitting annual participation elections'], \\\n",
    "    'a4b':['b. proposals for permitting election of differing levels of risk within the basic track\\'s glide path', 'c. proposals for permitting annual election of beneficiary assignment methodology'], \\\n",
    "    'a4c':['c. proposals for permitting annual election of beneficiary assignment methodology', '5. determining participation options based on medicare ffs revenue and prior participation '], \\\n",
    "    'a5b':['b. differentiating between low revenue acos and high revenue acos', 'c. determining participation options based on prior participation of aco legal entity and aco participants'], \\\n",
    "    'a5c':['c. determining participation options based on prior participation of aco legal entity and aco participants', 'd. monitoring for financial performance'], \\\n",
    "    'a5d': ['d. monitoring for financial performance', '6. requirements for aco participation in two-sided models'], \\\n",
    "    'a6b': ['b. election of msr/mlr by acos', 'c. aco repayment mechanisms'], \\\n",
    "    'a6c': ['c. aco repayment mechanisms', 'd. advance notice for and payment consequences of termination '], \\\n",
    "    'a6d2': ['(2) proposals for advance notice of voluntary termination', '(3) proposals for payment consequences of termination'], \\\n",
    "    'a6d3': ['(3) proposals for payment consequences of termination', '7. participation options for agreement periods beginning in 2019'], \\\n",
    "    'a7b': ['b. methodology for determining financial and quality performance for the 6-month performance years during 2019', 'c. applicability of program policies to acos participating in a 6-month performance year'], \\\n",
    "    'a7c': ['c. applicability of program policies to acos participating in a 6-month performance year', 'b. fee-for-service benefit enhancements'], \\\n",
    "    'b2a': ['a. shared savings program snf 3-day rule waiver', 'b. billing and payment for telehealth services'], \\\n",
    "    'b2b': ['b. billing and payment for telehealth services', 'c. providing tools to strengthen beneficiary engagement'], \\\n",
    "    'c2': ['2. beneficiary incentives', '3. empowering beneficiary choice'], \\\n",
    "    'c3a': ['3. empowering beneficiary choice', 'b. beneficiary opt-in based assignment methodology'], \\\n",
    "    'c3b': ['b. beneficiary opt-in based assignment methodology', 'd. benchmarking methodology refinements'],  \\\n",
    "    'd2': ['2. risk adjustment methodology for adjusting historical benchmark each performance year', '3. use of regional factors when establishing and resetting acos\\' benchmarks'], \\\n",
    "    'd3b': ['b. proposals to apply regional expenditures in determining the benchmark for an aco\\'s first agreement period', 'c. proposals for modifying the regional adjustment'], \\\n",
    "    'd3c': ['c. proposals for modifying the regional adjustment', 'd. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark'], \\\n",
    "    'd3d': ['d. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark', '4. technical changes to incorporate references to benchmark rebasing policies'], \\\n",
    "    'd4': ['4. technical changes to incorporate references to benchmark rebasing policies', 'e. updating program policies'], \\\n",
    "    'e2': ['2. revisions to policies on voluntary alignment', '3. revisions to the definition of primary care services used in beneficiary assignment'], \\\n",
    "    'e3': ['3. revisions to the definition of primary care services used in beneficiary assignment', '4. extreme and uncontrollable circumstances policies for the shared savings program'], \\\n",
    "    'e4': ['4. extreme and uncontrollable circumstances policies for the shared savings program', '5. program data and quality measures'], \\\n",
    "    'e5':['5. program data and quality measures', '6. promoting interoperability'], \\\n",
    "    'e6':['6. promoting interoperability', '7. coordination of pharmacy care for aco beneficiaries'], \\\n",
    "    'e7':['7. coordination of pharmacy care for aco beneficiaries', 'f. applicability of proposed policies to track 1+ model acos'], \\\n",
    "    'f2':['2. unavailability of application cycles for entry into the track 1+ model in 2019 and 2020', '3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements'], \\\n",
    "    'f3':['3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements', 'g. summary of proposed timing of applicability']}\n",
    "    \n",
    "    for key, value in startdict.items():    \n",
    "       splitlist = splitlist.split(value[0]) #split on start of desired section\n",
    "       split_further = splitlist[1].split(value[1]) #split again on start of undesired section\n",
    "       rulechunks[key] = {\"text\": split_further[0]} #choose only first half to upload to dict\n",
    "       splitlist = splitlist[1] #choose second half to prepare for next split\n",
    "    \n",
    "    #print(rulechunks.keys())   \n",
    "    #print(rulechunks[\"f3\"])\n",
    "    \n",
    "    #lengths = [len(chunk) for chunk in rulechunks.values()]\n",
    "    #print(lengths) #characters\n",
    "    \n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_chars(rule_url):\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    #chunk by num chars\n",
    "    chunklist = [(splitlist[i:i+8000]) for i in range(0, len(splitlist), 8000)]\n",
    "    \n",
    "    #make dict\n",
    "    rulechunks = {}\n",
    "    keys = range(len(chunklist))\n",
    "    for i in keys:\n",
    "        rulechunks[i] = {\"text\": chunklist[i]}\n",
    "\n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headchars(rule_url):\n",
    "    chunks = splitRule_headers(rule_url) #call first method\n",
    "    for key, value in chunks.items():\n",
    "        if len(value[\"text\"]) > 50000: #check to see if chunk is really long\n",
    "            smalldict = {}\n",
    "            chunklist = [value[\"text\"][i:i+25000] for i in range(0, len(value[\"text\"]), 25000)] #split chunk\n",
    "            keys = range(len(chunklist))\n",
    "            for i in keys:\n",
    "                smalldict[key+str(i)] = {\"text\": chunklist[i]} #add to sub-dictionary\n",
    "            chunks[key] = smalldict #add to big dictionary\n",
    "        else:\n",
    "            chunks[key] = {key+str(0): value} #add to big dictionary\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Rule Splits to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesplit_toES(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={key:value}, doc_type='_doc')\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruletoES_hybrid(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "        for small_key, small_value in value.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={small_key:small_value})\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: 31\n"
     ]
    }
   ],
   "source": [
    "rulesplit_test = splitRule_headers(rule_url)\n",
    "test_upload = rulesplit_toES(rulesplit_test, \"headers_new\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: 111\n"
     ]
    }
   ],
   "source": [
    "rulesplit_test2 = splitRule_chars(rule_url)\n",
    "test_upload2 = rulesplit_toES(rulesplit_test2, \"char_8000\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: 90\n"
     ]
    }
   ],
   "source": [
    "rulesplit_test3 = splitRule_headchars(rule_url)\n",
    "test_upload3 = ruletoES_hybrid(rulesplit_test3, \"hybrid_new\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ES Search\n",
    "def ES_search(index, querydict):\n",
    "    search = es.search(index=index, doc_type=\"_doc\", body={\"query\": querydict})\n",
    "    test_list = []\n",
    "    if search['hits']['hits'] != []:\n",
    "        for h in search['hits']['hits']:\n",
    "            key = list((h['_source'].keys()))\n",
    "            test_list.append({key[0]:h['_score']})\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = {\"simple_query_string\": {\"query\": \"urban\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_test_search = ES_search(\"headers_new\", test_query)\n",
    "headers_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_test_search = ES_search(\"char_8000\", test_query)\n",
    "char_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_test_search = ES_search(\"hybrid_new\", test_query)\n",
    "hybrid_test_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete an index:\n",
    "# es.indices.delete(index='index_name', ignore=[400, 404])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
