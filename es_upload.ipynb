{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Splitting & ElasticSearch Uploads\n",
    "## LMI Capstone Team\n",
    "## Summer Chambers | Steve Morris | Kaleb Shikur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rule Splitting, Uploading to ES\n",
    "'''\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection, ElasticsearchException\n",
    "# from requests_aws4auth import AWS4Auth\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests #gets urls\n",
    "import time\n",
    "# from aws_requests_auth.boto_utils import BotoAWSRequestsAuth\n",
    "\n",
    "host = 'https://search-lmi-capstone-test-bbewbwsli4uchb7r24zujwrqne.us-east-1.es.amazonaws.com'\n",
    "# auth = BotoAWSRequestsAuth(aws_host=host,\n",
    "#                            aws_region='us-east-1',\n",
    "#                            aws_service='es')\n",
    "# awsauth = AWS4Auth(YOUR_ACCESS_KEY, YOUR_SECRET_KEY, REGION, 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dedbbb462bf60acdb6c63621c8bf117c', 'cluster_name': '846033058400:lmi-capstone-test', 'cluster_uuid': 'vhqZptiWQEq0BVgaN6qi8Q', 'version': {'number': '7.8.0', 'build_flavor': 'oss', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2020-10-15T22:09:28.519938Z', 'build_snapshot': False, 'lucene_version': '8.5.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(host, timeout = 45)\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "print(es.info())\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     http_auth=auth,\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "# print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Rule Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Rule/CMS-2018-0101-0001.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headers(rule_url):\n",
    "\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    rulechunks = {}\n",
    "    \n",
    "    startdict = {'a2':['2. proposals for modified participation options under 5-year agreement periods', '3. creating a basic track with glide path to performance-based risk'], \\\n",
    "     'a3':['3. creating a basic track with glide path to performance-based risk', '4. permitting annual participation elections'], \\\n",
    "    'a4b':['b. proposals for permitting election of differing levels of risk within the basic track\\'s glide path', 'c. proposals for permitting annual election of beneficiary assignment methodology'], \\\n",
    "    'a4c':['c. proposals for permitting annual election of beneficiary assignment methodology', '5. determining participation options based on medicare ffs revenue and prior participation '], \\\n",
    "    'a5b':['b. differentiating between low revenue acos and high revenue acos', 'c. determining participation options based on prior participation of aco legal entity and aco participants'], \\\n",
    "    'a5c':['c. determining participation options based on prior participation of aco legal entity and aco participants', 'd. monitoring for financial performance'], \\\n",
    "    'a5d': ['d. monitoring for financial performance', '6. requirements for aco participation in two-sided models'], \\\n",
    "    'a6b': ['b. election of msr/mlr by acos', 'c. aco repayment mechanisms'], \\\n",
    "    'a6c': ['c. aco repayment mechanisms', 'd. advance notice for and payment consequences of termination '], \\\n",
    "    'a6d2': ['(2) proposals for advance notice of voluntary termination', '(3) proposals for payment consequences of termination'], \\\n",
    "    'a6d3': ['(3) proposals for payment consequences of termination', '7. participation options for agreement periods beginning in 2019'], \\\n",
    "    'a7b': ['b. methodology for determining financial and quality performance for the 6-month performance years during 2019', 'c. applicability of program policies to acos participating in a 6-month performance year'], \\\n",
    "    'a7c': ['c. applicability of program policies to acos participating in a 6-month performance year', 'b. fee-for-service benefit enhancements'], \\\n",
    "    'b2a': ['a. shared savings program snf 3-day rule waiver', 'b. billing and payment for telehealth services'], \\\n",
    "    'b2b': ['b. billing and payment for telehealth services', 'c. providing tools to strengthen beneficiary engagement'], \\\n",
    "    'c2': ['2. beneficiary incentives', '3. empowering beneficiary choice'], \\\n",
    "    'c3a': ['3. empowering beneficiary choice', 'b. beneficiary opt-in based assignment methodology'], \\\n",
    "    'c3b': ['b. beneficiary opt-in based assignment methodology', 'd. benchmarking methodology refinements'],  \\\n",
    "    'd2': ['2. risk adjustment methodology for adjusting historical benchmark each performance year', '3. use of regional factors when establishing and resetting acos\\' benchmarks'], \\\n",
    "    'd3b': ['b. proposals to apply regional expenditures in determining the benchmark for an aco\\'s first agreement period', 'c. proposals for modifying the regional adjustment'], \\\n",
    "    'd3c': ['c. proposals for modifying the regional adjustment', 'd. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark'], \\\n",
    "    'd3d': ['d. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark', '4. technical changes to incorporate references to benchmark rebasing policies'], \\\n",
    "    'd4': ['4. technical changes to incorporate references to benchmark rebasing policies', 'e. updating program policies'], \\\n",
    "    'e2': ['2. revisions to policies on voluntary alignment', '3. revisions to the definition of primary care services used in beneficiary assignment'], \\\n",
    "    'e3': ['3. revisions to the definition of primary care services used in beneficiary assignment', '4. extreme and uncontrollable circumstances policies for the shared savings program'], \\\n",
    "    'e4': ['4. extreme and uncontrollable circumstances policies for the shared savings program', '5. program data and quality measures'], \\\n",
    "    'e5':['5. program data and quality measures', '6. promoting interoperability'], \\\n",
    "    'e6':['6. promoting interoperability', '7. coordination of pharmacy care for aco beneficiaries'], \\\n",
    "    'e7':['7. coordination of pharmacy care for aco beneficiaries', 'f. applicability of proposed policies to track 1+ model acos'], \\\n",
    "    'f2':['2. unavailability of application cycles for entry into the track 1+ model in 2019 and 2020', '3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements'], \\\n",
    "    'f3':['3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements', 'g. summary of proposed timing of applicability']}\n",
    "    \n",
    "    for key, value in startdict.items():    \n",
    "       splitlist = splitlist.split(value[0]) #split on start of desired section\n",
    "       split_further = splitlist[1].split(value[1]) #split again on start of undesired section\n",
    "       rulechunks[key] = {\"text\": value[0]+split_further[0]} #choose only first half to upload to dict\n",
    "       splitlist = splitlist[1] #choose second half to prepare for next split\n",
    "    \n",
    "    #print(rulechunks.keys())   \n",
    "    #print(rulechunks[\"f3\"])\n",
    "    \n",
    "    #lengths = [len(chunk) for chunk in rulechunks.values()]\n",
    "    #print(lengths) #characters\n",
    "    \n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated Character-Count Split\n",
    "\n",
    "def splitRule_line_chars(rule_url):\n",
    "    #get rule\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "\n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    #Split on new paragraphs\n",
    "    paragraphs = splitlist.split('\\r\\n')\n",
    "    \n",
    "    #add new lines while under 2000 characters\n",
    "    for i in range(len(paragraphs) - 1):\n",
    "        while i < (len(paragraphs)-1) and len(paragraphs[i]) < 2000:\n",
    "            paragraphs[i] += paragraphs[i+1]\n",
    "            del(paragraphs[i+1])\n",
    "\n",
    "    #add to dictionary\n",
    "    rulechunks = {}\n",
    "    keys = range(len(paragraphs))\n",
    "    for i in keys:\n",
    "        rulechunks[i] = {\"text\": paragraphs[i]}\n",
    "\n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitRule_line_chars(rule_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_chars(rule_url):\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    #chunk by num chars\n",
    "    #chunklist = []\n",
    "    \n",
    "    chunklist = [(splitlist[i:i+8000]) for i in range(0, len(splitlist), 8000)]\n",
    "    \n",
    "    #make dict\n",
    "    rulechunks = {}\n",
    "    keys = range(len(chunklist))\n",
    "    for i in keys:\n",
    "        rulechunks[i] = {\"text\": chunklist[i]}\n",
    "\n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headchars(rule_url):\n",
    "    chunks = splitRule_headers(rule_url) #call first method\n",
    "    for key, value in chunks.items():\n",
    "        if len(value[\"text\"]) > 50000: #check to see if chunk is really long\n",
    "            smalldict = {}\n",
    "            chunklist = [value[\"text\"][i:i+25000] for i in range(0, len(value[\"text\"]), 25000)] #split chunk\n",
    "            keys = range(len(chunklist))\n",
    "            for i in keys:\n",
    "                smalldict[key+str(i)] = {\"text\": chunklist[i]} #add to sub-dictionary\n",
    "            chunks[key] = smalldict #add to big dictionary\n",
    "        else:\n",
    "            chunks[key] = {key+str(0): value} #add to big dictionary\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Rule Splits to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesplit_toES(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={key:value}, doc_type='_doc')\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruletoES_hybrid(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "        for small_key, small_value in value.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={small_key:small_value})\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesplit_test = splitRule_headers(rule_url)\n",
    "test_upload = rulesplit_toES(rulesplit_test, \"headers_new\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesplit_test2 = splitRule_chars(rule_url)\n",
    "test_upload2 = rulesplit_toES(rulesplit_test2, \"char_8000\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesplit_test3 = splitRule_headchars(rule_url)\n",
    "test_upload3 = ruletoES_hybrid(rulesplit_test3, \"hybrid_new\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ES Search\n",
    "def ES_search(index, querydict):\n",
    "    search = es.search(index=index, doc_type=\"_doc\", body={\"query\": querydict})\n",
    "    test_list = {}\n",
    "    if search['hits']['hits'] != []:\n",
    "        for h in search['hits']['hits']:\n",
    "            key = list((h['_source'].keys()))\n",
    "            test_list[key[0]]=h['_score']\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = {\"simple_query_string\": {\"query\": \"urban\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a6c': 0.45770907, 'a4c': 0.45549875, 'b2b': 0.29446962, 'c2': 0.2554381}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_test_search = ES_search(\"headers_new\", test_query)\n",
    "headers_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'64': 0.29684716, '70': 0.29528618, '68': 0.29148874}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_test_search = ES_search(\"char_8000\", test_query)\n",
    "char_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c20': 0.40307403,\n",
       " 'a4c15': 0.29141787,\n",
       " 'a6c7': 0.29115358,\n",
       " 'a6c8': 0.29011896,\n",
       " 'a4c16': 0.28976172,\n",
       " 'a4c17': 0.289463,\n",
       " 'a6c9': 0.28831965,\n",
       " 'b2b0': 0.1865641}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_test_search = ES_search(\"hybrid_new\", test_query)\n",
    "hybrid_test_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete an index:\n",
    "# es.indices.delete(index='index_name', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Comment Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comments(comment_url):\n",
    "    comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\"\n",
    "    response = requests.get(comment_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    a_tags = soup.findAll(\"a\")\n",
    "    links = [tag[\"href\"] for tag in a_tags]\n",
    "    txt_links = [link for link in links if '.txt' in link]\n",
    "    comments = {}\n",
    "    for suffix in txt_links:\n",
    "        comments[suffix] = requests.get(comment_url+suffix).text.lower()\n",
    "        #print(f\"scraping comment {suffix}\")\n",
    "    print(f\"scraped {len(txt_links)} comments\")\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped 674 comments\n"
     ]
    }
   ],
   "source": [
    "comments2018 = retrieve_comments(comment_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_lengths = {key: len(value) for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_comment_lengths = dict(sorted(comment_lengths.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_short = {key: len(value) for key, value in comments2018.items() if \"attach\" in value and len(value) < 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {(key[14:20]): value for key, value in comments2018.items()}\n",
    "sorted_keys = sorted(list(comments2018.keys()))\n",
    "\n",
    "# now for each key in the list\n",
    "for i in range(len(comments2018)-1):\n",
    "    # get key at index i and key at index i+1 and compare them\n",
    "    if sorted_keys[i+1][0:4] == sorted_keys[i][0:4]:\n",
    "        comments2018[sorted_keys[i+1]] = comments2018[sorted_keys[i]] + comments2018[sorted_keys[i+1]]\n",
    "        del(comments2018[sorted_keys[i]])\n",
    "\n",
    "#{key: len(value) for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {key[0:4]:value for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json = json.dumps(comments2018)\n",
    "f = open('comments2018.json','w')\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 10, 'successful': 5, 'failed': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index='headers_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 10, 'successful': 5, 'failed': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index='hybrid_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment001_query = {\"simple_query_string\": {\"query\": comments2018[\"0002\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4c': 144.72498,\n",
       " 'a6c': 142.13535,\n",
       " 'a5c': 110.61184,\n",
       " 'c3b': 106.4453,\n",
       " 'a3': 105.05455,\n",
       " 'a5b': 104.90532,\n",
       " 'a2': 103.80479,\n",
       " 'e4': 100.2969,\n",
       " 'a7c': 97.574455,\n",
       " 'a7b': 93.47521}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"headers_new\", test_comment001_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c20': 105.905975,\n",
       " 'a5b0': 104.90532,\n",
       " 'a20': 103.80479,\n",
       " 'e40': 100.2969,\n",
       " 'a6c19': 99.05433,\n",
       " 'a4c27': 98.69642,\n",
       " 'a5c1': 98.16931,\n",
       " 'a4c7': 97.86486,\n",
       " 'a4c9': 97.61569,\n",
       " 'a7c0': 97.574455}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"hybrid_new\", test_comment001_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_first3 = {key:value for key,value in comments2018}\n",
    "dict_first3 = {}\n",
    "for key in list(comments2018.keys())[0:3]:\n",
    "    dict_first3[key] = comments2018[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 maximum clause error at index 0003\n",
      "676 677 "
     ]
    }
   ],
   "source": [
    "# Test HYBRID on dictionary of 3 key/value\n",
    "\n",
    "results = {}\n",
    "for key, value in dict_first3.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"hybrid_new\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='hybrid_new')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HEADERS on dictionary of 3 key/value\n",
    "\n",
    "results = {}\n",
    "for key, value in dict_first3.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"headers_new\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='headers_new')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 maximum clause error at index 0012\n",
      "10 maximum clause error at index 0013\n",
      "11 maximum clause error at index 0014\n",
      "12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 maximum clause error at index 0055\n",
      "53 maximum clause error at index 0056\n",
      "54 maximum clause error at index 0057\n",
      "55 maximum clause error at index 0058\n",
      "56 maximum clause error at index 0059\n",
      "57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 maximum clause error at index 0081\n",
      "79 maximum clause error at index 0082\n",
      "80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 maximum clause error at index 0138\n",
      "136 137 maximum clause error at index 0140\n",
      "138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 maximum clause error at index 0158\n",
      "156 maximum clause error at index 0159\n",
      "157 158 maximum clause error at index 0161\n",
      "159 maximum clause error at index 0162\n",
      "160 maximum clause error at index 0163\n",
      "161 maximum clause error at index 0164\n",
      "162 maximum clause error at index 0165\n",
      "163 164 165 166 167 168 169 maximum clause error at index 0172\n",
      "170 maximum clause error at index 0173\n",
      "171 maximum clause error at index 0174\n"
     ]
    },
    {
     "ename": "TransportError",
     "evalue": "TransportError(503, '{\\n  \"message\": \"No server available to handle the request\",\\n}\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTransportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-09b4df4daa62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hybrid_new'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\client\\utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\client\\indices.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self, index, params, headers)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mignored\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0munavailable\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \"\"\"\n\u001b[1;32m---> 58\u001b[1;33m         return self.transport.perform_request(\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[1;34m\"POST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_refresh\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[0;32m    388\u001b[0m                     \u001b[1;31m# raise exception on last retry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m                 status, headers_response, data = connection.perform_request(\n\u001b[0m\u001b[0;32m    359\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\connection\\http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             )\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         self.log_request_success(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\connection\\base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[1;34m(self, status_code, raw_data)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Undecodable raw error response from server: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         )\n",
      "\u001b[1;31mTransportError\u001b[0m: TransportError(503, '{\\n  \"message\": \"No server available to handle the request\",\\n}\\n')"
     ]
    }
   ],
   "source": [
    "# HYBRID Full dictionary Query\n",
    "\n",
    "results = {}\n",
    "i=0\n",
    "for key, value in comments2018.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"hybrid_new\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='hybrid_new')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture HYBRID results in CSV\n",
    "import csv\n",
    "w = csv.writer(open('hybrid_results.csv', 'w'))\n",
    "for key, val in results.items():\n",
    "    w.writerow([key,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture HYBRID results in JSON\n",
    "import json\n",
    "json = json.dumps(results)\n",
    "f = open('hybrid_results.json','w')\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEADERS Full dictionary Query\n",
    "\n",
    "results = {}\n",
    "i=0\n",
    "for key, value in comments2018.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"headers_new\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='headers_new')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "w = csv.writer(open('header_results.csv', 'w'))\n",
    "for key, val in results.items():\n",
    "    w.writerow([key,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json = json.dumps(results)\n",
    "f = open('header_results.json','w')\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0002': ['a4c', 'a6c', 'a5c', 'c3b', 'a3', 'a5b', 'a2', 'e4', 'a7c', 'a7b'],\n",
       " '0003': ['a4c', 'a6c', 'a3', 'a5c', 'e4', 'c3b', 'a5b', 'a2', 'a7c', 'a7b'],\n",
       " '0004': ['a4c', 'a6c', 'a5c', 'a3', 'e4', 'a7c', 'a2', 'a5b', 'c3b', 'e6']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
