{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Splitting & ElasticSearch Uploads\n",
    "## LMI Capstone Team\n",
    "## Summer Chambers | Steve Morris | Kaleb Shikur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rule Splitting, Uploading to ES\n",
    "'''\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection, ElasticsearchException\n",
    "# from requests_aws4auth import AWS4Auth\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests #gets urls\n",
    "import time\n",
    "# from aws_requests_auth.boto_utils import BotoAWSRequestsAuth\n",
    "# 173.79.72.92, port 9200\n",
    "host = '173.79.72.92'\n",
    "# auth = BotoAWSRequestsAuth(aws_host=host,\n",
    "#                            aws_region='us-east-1',\n",
    "#                            aws_service='es')\n",
    "# awsauth = AWS4Auth(YOUR_ACCESS_KEY, YOUR_SECRET_KEY, REGION, 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'DESKTOP-OEAPD0P', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'ut8MVoyvRsiLoOmQV5B3dA', 'version': {'number': '7.10.0', 'build_flavor': 'default', 'build_type': 'zip', 'build_hash': '51e9d6f22758d0374a0f3f5c6e8f3a7997850f96', 'build_date': '2020-11-09T21:30:33.964949Z', 'build_snapshot': False, 'lucene_version': '8.7.0', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(host, timeout = 45)\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "print(es.info())\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     http_auth=auth,\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "# print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Rule Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Rule/CMS-2018-0101-0001.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headers(rule_url):\n",
    "\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    rulechunks = {}\n",
    "    \n",
    "    startdict = {'a2':['2. proposals for modified participation options under 5-year agreement periods', '3. creating a basic track with glide path to performance-based risk'], \\\n",
    "     'a3':['3. creating a basic track with glide path to performance-based risk', '4. permitting annual participation elections'], \\\n",
    "    'a4b':['b. proposals for permitting election of differing levels of risk within the basic track\\'s glide path', 'c. proposals for permitting annual election of beneficiary assignment methodology'], \\\n",
    "    'a4c':['c. proposals for permitting annual election of beneficiary assignment methodology', '5. determining participation options based on medicare ffs revenue and prior participation '], \\\n",
    "    'a5b':['b. differentiating between low revenue acos and high revenue acos', 'c. determining participation options based on prior participation of aco legal entity and aco participants'], \\\n",
    "    'a5c':['c. determining participation options based on prior participation of aco legal entity and aco participants', 'd. monitoring for financial performance'], \\\n",
    "    'a5d': ['d. monitoring for financial performance', '6. requirements for aco participation in two-sided models'], \\\n",
    "    'a6b': ['b. election of msr/mlr by acos', 'c. aco repayment mechanisms'], \\\n",
    "    'a6c': ['c. aco repayment mechanisms', 'd. advance notice for and payment consequences of termination '], \\\n",
    "    'a6d2': ['(2) proposals for advance notice of voluntary termination', '(3) proposals for payment consequences of termination'], \\\n",
    "    'a6d3': ['(3) proposals for payment consequences of termination', '7. participation options for agreement periods beginning in 2019'], \\\n",
    "    'a7b': ['b. methodology for determining financial and quality performance for the 6-month performance years during 2019', 'c. applicability of program policies to acos participating in a 6-month performance year'], \\\n",
    "    'a7c': ['c. applicability of program policies to acos participating in a 6-month performance year', 'b. fee-for-service benefit enhancements'], \\\n",
    "    'b2a': ['a. shared savings program snf 3-day rule waiver', 'b. billing and payment for telehealth services'], \\\n",
    "    'b2b': ['b. billing and payment for telehealth services', 'c. providing tools to strengthen beneficiary engagement'], \\\n",
    "    'c2': ['2. beneficiary incentives', '3. empowering beneficiary choice'], \\\n",
    "    'c3a': ['3. empowering beneficiary choice', 'b. beneficiary opt-in based assignment methodology'], \\\n",
    "    'c3b': ['b. beneficiary opt-in based assignment methodology', 'd. benchmarking methodology refinements'],  \\\n",
    "    'd2': ['2. risk adjustment methodology for adjusting historical benchmark each performance year', '3. use of regional factors when establishing and resetting acos\\' benchmarks'], \\\n",
    "    'd3b': ['b. proposals to apply regional expenditures in determining the benchmark for an aco\\'s first agreement period', 'c. proposals for modifying the regional adjustment'], \\\n",
    "    'd3c': ['c. proposals for modifying the regional adjustment', 'd. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark'], \\\n",
    "    'd3d': ['d. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark', '4. technical changes to incorporate references to benchmark rebasing policies'], \\\n",
    "    'd4': ['4. technical changes to incorporate references to benchmark rebasing policies', 'e. updating program policies'], \\\n",
    "    'e2': ['2. revisions to policies on voluntary alignment', '3. revisions to the definition of primary care services used in beneficiary assignment'], \\\n",
    "    'e3': ['3. revisions to the definition of primary care services used in beneficiary assignment', '4. extreme and uncontrollable circumstances policies for the shared savings program'], \\\n",
    "    'e4': ['4. extreme and uncontrollable circumstances policies for the shared savings program', '5. program data and quality measures'], \\\n",
    "    'e5':['5. program data and quality measures', '6. promoting interoperability'], \\\n",
    "    'e6':['6. promoting interoperability', '7. coordination of pharmacy care for aco beneficiaries'], \\\n",
    "    'e7':['7. coordination of pharmacy care for aco beneficiaries', 'f. applicability of proposed policies to track 1+ model acos'], \\\n",
    "    'f2':['2. unavailability of application cycles for entry into the track 1+ model in 2019 and 2020', '3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements'], \\\n",
    "    'f3':['3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements', 'g. summary of proposed timing of applicability']}\n",
    "    \n",
    "    for key, value in startdict.items():    \n",
    "       splitlist = splitlist.split(value[0]) #split on start of desired section\n",
    "       split_further = splitlist[1].split(value[1]) #split again on start of undesired section\n",
    "       rulechunks[key] = {\"text\": value[0]+split_further[0]} #choose only first half to upload to dict\n",
    "       splitlist = splitlist[1] #choose second half to prepare for next split\n",
    "    \n",
    "    #print(rulechunks.keys())   \n",
    "    #print(rulechunks[\"f3\"])\n",
    "    \n",
    "    #lengths = [len(chunk) for chunk in rulechunks.values()]\n",
    "    #print(lengths) #characters\n",
    "    \n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated Character-Count Split\n",
    "\n",
    "def splitRule_line_chars(rule_url):\n",
    "    #get rule\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "\n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    #Split on new paragraphs\n",
    "    paragraphs = splitlist.split('\\r\\n')\n",
    "    \n",
    "    #add new lines while under 2000 characters\n",
    "    for i in range(len(paragraphs) - 1):\n",
    "        while i < (len(paragraphs)-1) and len(paragraphs[i]) < 2000:\n",
    "            paragraphs[i] += paragraphs[i+1]\n",
    "            del(paragraphs[i+1])\n",
    "\n",
    "    #add to dictionary\n",
    "    rulechunks = {}\n",
    "    keys = range(len(paragraphs))\n",
    "    for i in keys:\n",
    "        rulechunks[i] = {\"text\": paragraphs[i]}\n",
    "\n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitchars = splitRule_line_chars(rule_url)\n",
    "{key: len(value2) for key, value in splitchars.items() for key2, value2 in value.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_chars(rule_url):\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    #chunk by num chars\n",
    "    #chunklist = []\n",
    "    \n",
    "    chunklist = [(splitlist[i:i+8000]) for i in range(0, len(splitlist), 8000)]\n",
    "    \n",
    "    #make dict\n",
    "    rulechunks = {}\n",
    "    keys = range(len(chunklist))\n",
    "    for i in keys:\n",
    "        rulechunks[i] = {\"text\": chunklist[i]}\n",
    "\n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headchars(rule_url):\n",
    "    chunks = splitRule_headers(rule_url) #call first method\n",
    "    for key, value in chunks.items():\n",
    "        if len(value[\"text\"]) > 50000: #check to see if chunk is really long\n",
    "            smalldict = {}\n",
    "            chunklist = [value[\"text\"][i:i+25000] for i in range(0, len(value[\"text\"]), 25000)] #split chunk\n",
    "            keys = range(len(chunklist))\n",
    "            for i in keys:\n",
    "                smalldict[key+str(i)] = {\"text\": chunklist[i]} #add to sub-dictionary\n",
    "            chunks[key] = smalldict #add to big dictionary\n",
    "        else:\n",
    "            chunks[key] = {key+str(0): value} #add to big dictionary\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_line_hybrid(rule_url):\n",
    "    new_rule_chunks = {}\n",
    "    chunks = splitRule_headers(rule_url)\n",
    "    for key, value in chunks.items():\n",
    "        for text, real_value in value.items():\n",
    "            paragraphs = real_value.split('\\r\\n')\n",
    "            #add new lines while under 2000 characters\n",
    "            for i in range(len(paragraphs) - 1):\n",
    "                while i < (len(paragraphs)-1) and len(paragraphs[i]) < 2000:\n",
    "                    paragraphs[i] += paragraphs[i+1]\n",
    "                    del(paragraphs[i+1])\n",
    "            for i in range(len(paragraphs)):\n",
    "                new_rule_chunks[key+str(i)] = {text: paragraphs[i]}\n",
    "    return new_rule_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = splitRule_line_hybrid(rule_url)\n",
    "lengths = {key: len(small_value) for key, value in result.items() for small_key, small_value in value.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Rule Splits to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesplit_toES(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={key:value}, doc_type='_doc')\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruletoES_hybrid(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "        for small_key, small_value in value.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={small_key:small_value})\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesplit_test = splitRule_headers(rule_url)\n",
    "test_upload = rulesplit_toES(rulesplit_test, \"headers_new\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: 31\n"
     ]
    }
   ],
   "source": [
    "rulesplit_test2 = splitRule_headchars(rule_url)\n",
    "test_upload2 = rulesplit_toES(rulesplit_test2, \"char_25000\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: 783\n"
     ]
    }
   ],
   "source": [
    "rulesplit_test3 = splitRule_line_hybrid(rule_url)\n",
    "test_upload3 = ruletoES_hybrid(rulesplit_test3, \"hybrid_2000\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ES Search\n",
    "def ES_search(index, querydict):\n",
    "    search = es.search(index=index, doc_type=\"_doc\", body={\"query\": querydict})\n",
    "    test_list = {}\n",
    "    if search['hits']['hits'] != []:\n",
    "        for h in search['hits']['hits']:\n",
    "            key = list((h['_source'].keys()))\n",
    "            test_list[key[0]]=h['_score']\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = {\"simple_query_string\": {\"query\": \"urban\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\connection\\base.py:190: ElasticsearchDeprecationWarning: [types removal] Specifying types in search requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a6c': 0.45770907, 'a4c': 0.45549875, 'c2': 0.40307403, 'b2b': 0.29446962}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_test_search = ES_search(\"headers_new\", test_query)\n",
    "headers_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4c': 0.8706426, 'a6c': 0.8695922, 'c2': 0.40307403, 'b2b': 0.29446962}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_test_search = ES_search(\"char_25000\", test_query)\n",
    "char_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 4.1722136}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_test_search = ES_search(\"hybrid_2000\", test_query)\n",
    "hybrid_test_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete an index:\n",
    "# es.indices.delete(index='index_name', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Comment Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comments(comment_url):\n",
    "    comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\"\n",
    "    response = requests.get(comment_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    a_tags = soup.findAll(\"a\")\n",
    "    links = [tag[\"href\"] for tag in a_tags]\n",
    "    txt_links = [link for link in links if '.txt' in link]\n",
    "    comments = {}\n",
    "    for suffix in txt_links:\n",
    "        comments[suffix] = requests.get(comment_url+suffix).text.lower()\n",
    "        #print(f\"scraping comment {suffix}\")\n",
    "    print(f\"scraped {len(txt_links)} comments\")\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped 674 comments\n"
     ]
    }
   ],
   "source": [
    "comments2018 = retrieve_comments(comment_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_lengths = {key: len(value) for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_comment_lengths = dict(sorted(comment_lengths.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_short = {key: len(value) for key, value in comments2018.items() if \"attach\" in value and len(value) < 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {(key[14:20]): value for key, value in comments2018.items()}\n",
    "sorted_keys = sorted(list(comments2018.keys()))\n",
    "\n",
    "# now for each key in the list\n",
    "for i in range(len(comments2018)-1):\n",
    "    # get key at index i and key at index i+1 and compare them\n",
    "    if sorted_keys[i+1][0:4] == sorted_keys[i][0:4]:\n",
    "        comments2018[sorted_keys[i+1]] = comments2018[sorted_keys[i]] + comments2018[sorted_keys[i+1]]\n",
    "        del(comments2018[sorted_keys[i]])\n",
    "\n",
    "#{key: len(value) for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {key[0:4]:value for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comments2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json = json.dumps(comments2018)\n",
    "f = open('comments2018.json','w')\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index='headers_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index='hybrid_2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment001_query = {\"simple_query_string\": {\"query\": comments2018[\"0002\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4c': 144.72498,\n",
       " 'a6c': 142.13535,\n",
       " 'a5c': 110.61184,\n",
       " 'c3b': 106.4453,\n",
       " 'c2': 105.905975,\n",
       " 'a3': 105.05455,\n",
       " 'a5b': 104.90532,\n",
       " 'a2': 103.80479,\n",
       " 'e4': 100.2969,\n",
       " 'a7c': 97.574455}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"headers_new\", test_comment001_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4c': 2863.9329,\n",
       " 'a6c': 2104.7112,\n",
       " 'a5c': 371.47354,\n",
       " 'a3': 255.84105,\n",
       " 'c3b': 223.0071,\n",
       " 'c2': 105.905975,\n",
       " 'a5b': 104.90532,\n",
       " 'a2': 103.80479,\n",
       " 'e4': 100.2969,\n",
       " 'a7c': 97.574455}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"char_25000\", test_comment001_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 83.15839}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"hybrid_2000\", test_comment001_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_first3 = {key:value for key,value in comments2018}\n",
    "dict_first3 = {}\n",
    "for key in list(comments2018.keys())[0:3]:\n",
    "    dict_first3[key] = comments2018[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468 469 470 "
     ]
    }
   ],
   "source": [
    "# Test HYBRID on dictionary of 3 key/value\n",
    "\n",
    "results = {}\n",
    "for key, value in dict_first3.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"hybrid_2000\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='hybrid_2000')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HEADERS on dictionary of 3 key/value\n",
    "\n",
    "results = {}\n",
    "for key, value in dict_first3.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"headers_new\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='headers_new')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 472 473 "
     ]
    }
   ],
   "source": [
    "# Test CHARACTERS on dictionary of 3 key/value\n",
    "\n",
    "results = {}\n",
    "for key, value in dict_first3.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"char_25000\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='char_25000')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\anaconda3\\envs\\python385\\lib\\site-packages\\elasticsearch\\connection\\base.py:190: ElasticsearchDeprecationWarning: [types removal] Specifying types in search requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 maximum clause error at index 0012\n",
      "10 maximum clause error at index 0013\n",
      "11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 maximum clause error at index 0081\n",
      "79 maximum clause error at index 0082\n",
      "80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 maximum clause error at index 0138\n",
      "136 137 maximum clause error at index 0140\n",
      "138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 maximum clause error at index 0190\n",
      "188 189 190 191 192 193 194 maximum clause error at index 0197\n",
      "195 196 197 198 199 200 maximum clause error at index 0203\n",
      "201 maximum clause error at index 0204\n",
      "202 203 204 maximum clause error at index 0207\n",
      "205 maximum clause error at index 0208\n",
      "206 207 208 209 maximum clause error at index 0212\n",
      "210 211 212 213 maximum clause error at index 0216\n",
      "214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 maximum clause error at index 0234\n",
      "232 233 234 235 maximum clause error at index 0238\n",
      "236 237 238 maximum clause error at index 0241\n",
      "239 240 maximum clause error at index 0243\n",
      "241 maximum clause error at index 0244\n",
      "242 maximum clause error at index 0245\n",
      "243 244 maximum clause error at index 0247\n",
      "245 maximum clause error at index 0248\n",
      "246 247 maximum clause error at index 0250\n",
      "248 249 250 251 maximum clause error at index 0254\n",
      "252 253 254 255 256 257 maximum clause error at index 0260\n",
      "258 259 260 261 262 263 264 265 maximum clause error at index 0268\n",
      "266 maximum clause error at index 0269\n",
      "267 268 269 270 271 272 273 274 275 276 maximum clause error at index 0279\n",
      "277 278 279 280 281 282 283 maximum clause error at index 0286\n",
      "284 285 286 287 288 289 290 291 292 maximum clause error at index 0295\n",
      "293 maximum clause error at index 0296\n",
      "294 295 296 297 298 maximum clause error at index 0301\n",
      "299 300 301 maximum clause error at index 0304\n",
      "302 303 304 maximum clause error at index 0307\n",
      "305 maximum clause error at index 0308\n",
      "306 307 308 309 maximum clause error at index 0312\n",
      "310 maximum clause error at index 0313\n",
      "311 312 313 maximum clause error at index 0316\n",
      "314 maximum clause error at index 0317\n",
      "315 maximum clause error at index 0318\n",
      "316 317 maximum clause error at index 0320\n",
      "318 319 maximum clause error at index 0322\n",
      "320 321 322 323 maximum clause error at index 0326\n",
      "324 325 326 327 maximum clause error at index 0330\n",
      "328 329 330 maximum clause error at index 0333\n",
      "331 332 maximum clause error at index 0335\n",
      "333 334 335 336 337 338 maximum clause error at index 0341\n",
      "339 340 341 342 maximum clause error at index 0345\n",
      "343 maximum clause error at index 0346\n",
      "344 345 346 maximum clause error at index 0349\n",
      "347 maximum clause error at index 0350\n",
      "348 349 350 maximum clause error at index 0353\n",
      "351 352 353 354 maximum clause error at index 0357\n",
      "355 maximum clause error at index 0358\n",
      "356 357 358 359 360 361 362 maximum clause error at index 0365\n",
      "363 maximum clause error at index 0366\n",
      "364 maximum clause error at index 0367\n",
      "365 maximum clause error at index 0368\n",
      "366 maximum clause error at index 0369\n",
      "367 maximum clause error at index 0370\n",
      "368 369 maximum clause error at index 0372\n",
      "370 maximum clause error at index 0373\n",
      "371 372 maximum clause error at index 0375\n",
      "373 374 maximum clause error at index 0377\n",
      "375 maximum clause error at index 0378\n",
      "376 maximum clause error at index 0379\n",
      "377 378 maximum clause error at index 0381\n",
      "379 maximum clause error at index 0382\n",
      "380 maximum clause error at index 0383\n",
      "381 382 383 maximum clause error at index 0386\n",
      "384 maximum clause error at index 0387\n",
      "385 maximum clause error at index 0388\n",
      "386 maximum clause error at index 0389\n",
      "387 388 maximum clause error at index 0391\n",
      "389 maximum clause error at index 0392\n",
      "390 maximum clause error at index 0393\n",
      "391 392 maximum clause error at index 0395\n",
      "393 maximum clause error at index 0396\n",
      "394 395 maximum clause error at index 0398\n",
      "396 397 398 399 400 401 402 403 maximum clause error at index 0406\n",
      "404 405 406 maximum clause error at index 0409\n",
      "407 408 409 maximum clause error at index 0412\n",
      "410 maximum clause error at index 0413\n",
      "411 412 maximum clause error at index 0415\n",
      "413 maximum clause error at index 0416\n",
      "414 maximum clause error at index 0417\n",
      "415 maximum clause error at index 0418\n",
      "416 417 maximum clause error at index 0420\n",
      "418 maximum clause error at index 0421\n",
      "419 420 maximum clause error at index 0423\n",
      "421 maximum clause error at index 0424\n",
      "422 423 424 maximum clause error at index 0427\n",
      "425 426 427 428 429 430 maximum clause error at index 0433\n",
      "431 432 433 434 435 436 437 maximum clause error at index 0440\n",
      "438 maximum clause error at index 0441\n",
      "439 440 441 442 maximum clause error at index 0445\n",
      "443 444 445 446 maximum clause error at index 0449\n",
      "447 maximum clause error at index 0450\n",
      "448 449 maximum clause error at index 0452\n",
      "450 451 452 maximum clause error at index 0455\n",
      "453 maximum clause error at index 0456\n",
      "454 455 456 maximum clause error at index 0459\n",
      "457 maximum clause error at index 0460\n",
      "458 459 maximum clause error at index 0462\n",
      "460 461 462 463 464 465 466 maximum clause error at index 0469\n",
      "467 "
     ]
    }
   ],
   "source": [
    "# CHARACTER Full dictionary Query\n",
    "\n",
    "results = {}\n",
    "i=0\n",
    "for key, value in comments2018.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"char_25000\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='char_25000')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture CHARACTER results in CSV\n",
    "import csv\n",
    "w = csv.writer(open('char_results.csv', 'w'))\n",
    "for key, val in results.items():\n",
    "    w.writerow([key,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture CHARACTER results in JSON\n",
    "import json\n",
    "json = json.dumps(results)\n",
    "f = open('char_results.json','w')\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 maximum clause error at index 0012\n",
      "10 maximum clause error at index 0013\n",
      "11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 maximum clause error at index 0081\n",
      "79 maximum clause error at index 0082\n",
      "80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 maximum clause error at index 0138\n",
      "136 137 maximum clause error at index 0140\n",
      "138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 maximum clause error at index 0190\n",
      "188 189 190 191 192 193 194 maximum clause error at index 0197\n",
      "195 196 197 198 199 200 maximum clause error at index 0203\n",
      "201 maximum clause error at index 0204\n",
      "202 203 204 maximum clause error at index 0207\n",
      "205 maximum clause error at index 0208\n",
      "206 207 208 209 maximum clause error at index 0212\n",
      "210 211 212 213 maximum clause error at index 0216\n",
      "214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 maximum clause error at index 0234\n",
      "232 233 234 235 maximum clause error at index 0238\n",
      "236 237 238 maximum clause error at index 0241\n",
      "239 240 maximum clause error at index 0243\n",
      "241 maximum clause error at index 0244\n",
      "242 maximum clause error at index 0245\n",
      "243 244 maximum clause error at index 0247\n",
      "245 maximum clause error at index 0248\n",
      "246 247 maximum clause error at index 0250\n",
      "248 249 250 251 maximum clause error at index 0254\n",
      "252 253 254 255 256 257 maximum clause error at index 0260\n",
      "258 259 260 261 262 263 264 265 maximum clause error at index 0268\n",
      "266 maximum clause error at index 0269\n",
      "267 268 269 270 271 272 273 274 275 276 maximum clause error at index 0279\n",
      "277 278 279 280 281 282 283 maximum clause error at index 0286\n",
      "284 285 286 287 288 289 290 291 292 maximum clause error at index 0295\n",
      "293 maximum clause error at index 0296\n",
      "294 295 296 297 298 maximum clause error at index 0301\n",
      "299 300 301 maximum clause error at index 0304\n",
      "302 303 304 maximum clause error at index 0307\n",
      "305 maximum clause error at index 0308\n",
      "306 307 308 309 maximum clause error at index 0312\n",
      "310 maximum clause error at index 0313\n",
      "311 312 313 maximum clause error at index 0316\n",
      "314 maximum clause error at index 0317\n",
      "315 maximum clause error at index 0318\n",
      "316 317 maximum clause error at index 0320\n",
      "318 319 maximum clause error at index 0322\n",
      "320 321 322 323 maximum clause error at index 0326\n",
      "324 325 326 327 maximum clause error at index 0330\n",
      "328 329 330 maximum clause error at index 0333\n",
      "331 332 maximum clause error at index 0335\n",
      "333 334 335 336 337 338 maximum clause error at index 0341\n",
      "339 340 341 342 maximum clause error at index 0345\n",
      "343 maximum clause error at index 0346\n",
      "344 345 346 maximum clause error at index 0349\n",
      "347 maximum clause error at index 0350\n",
      "348 349 350 maximum clause error at index 0353\n",
      "351 352 353 354 maximum clause error at index 0357\n",
      "355 maximum clause error at index 0358\n",
      "356 357 358 359 360 361 362 maximum clause error at index 0365\n",
      "363 maximum clause error at index 0366\n",
      "364 maximum clause error at index 0367\n",
      "365 maximum clause error at index 0368\n",
      "366 maximum clause error at index 0369\n",
      "367 maximum clause error at index 0370\n",
      "368 369 maximum clause error at index 0372\n",
      "370 maximum clause error at index 0373\n",
      "371 372 maximum clause error at index 0375\n",
      "373 374 maximum clause error at index 0377\n",
      "375 maximum clause error at index 0378\n",
      "376 maximum clause error at index 0379\n",
      "377 378 maximum clause error at index 0381\n",
      "379 maximum clause error at index 0382\n",
      "380 maximum clause error at index 0383\n",
      "381 382 383 maximum clause error at index 0386\n",
      "384 maximum clause error at index 0387\n",
      "385 maximum clause error at index 0388\n",
      "386 maximum clause error at index 0389\n",
      "387 388 maximum clause error at index 0391\n",
      "389 maximum clause error at index 0392\n",
      "390 maximum clause error at index 0393\n",
      "391 392 maximum clause error at index 0395\n",
      "393 maximum clause error at index 0396\n",
      "394 395 maximum clause error at index 0398\n",
      "396 397 398 399 400 401 402 403 maximum clause error at index 0406\n",
      "404 405 406 maximum clause error at index 0409\n",
      "407 408 409 maximum clause error at index 0412\n",
      "410 maximum clause error at index 0413\n",
      "411 412 maximum clause error at index 0415\n",
      "413 maximum clause error at index 0416\n",
      "414 maximum clause error at index 0417\n",
      "415 maximum clause error at index 0418\n",
      "416 417 maximum clause error at index 0420\n",
      "418 maximum clause error at index 0421\n",
      "419 420 maximum clause error at index 0423\n",
      "421 maximum clause error at index 0424\n",
      "422 423 424 maximum clause error at index 0427\n",
      "425 426 427 428 429 430 maximum clause error at index 0433\n",
      "431 432 433 434 435 436 437 maximum clause error at index 0440\n",
      "438 maximum clause error at index 0441\n",
      "439 440 441 442 maximum clause error at index 0445\n",
      "443 444 445 446 maximum clause error at index 0449\n",
      "447 maximum clause error at index 0450\n",
      "448 449 maximum clause error at index 0452\n",
      "450 451 452 maximum clause error at index 0455\n",
      "453 maximum clause error at index 0456\n",
      "454 455 456 maximum clause error at index 0459\n",
      "457 maximum clause error at index 0460\n",
      "458 459 maximum clause error at index 0462\n",
      "460 461 462 463 464 465 466 maximum clause error at index 0469\n",
      "467 "
     ]
    }
   ],
   "source": [
    "# HEADERS Full dictionary Query\n",
    "\n",
    "results = {}\n",
    "i=0\n",
    "for key, value in comments2018.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"headers_new\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='headers_new')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "w = csv.writer(open('header_results2.csv', 'w'))\n",
    "for key, val in results.items():\n",
    "    w.writerow([key,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json = json.dumps(results)\n",
    "f = open('header_results2.json','w')\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYBRID Full dictionary Query\n",
    "\n",
    "results = {}\n",
    "i=0\n",
    "for key, value in comments2018.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"hybrid_2000\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='hybrid_2000')\n",
    "    print(i, end=' ')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture HYBRID results in CSV\n",
    "import csv\n",
    "w = csv.writer(open('hybrid_results.csv', 'w'))\n",
    "for key, val in results.items():\n",
    "    w.writerow([key,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture HYBRID results in JSON\n",
    "import json\n",
    "json = json.dumps(results)\n",
    "f = open('hybrid_results.json','w')\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
